{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8b319b-b0e1-4bd3-88fa-0eeeb0ca16fe",
   "metadata": {},
   "source": [
    "### Code Build using \n",
    "- Anaconda Navigator\n",
    "- Jupyter Notebook\n",
    "- Python and python libaries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d926371-5a9e-43c1-85f8-95b131d25183",
   "metadata": {},
   "source": [
    "### Data Processing \n",
    "- Generate a csv file containing first_name, last_name, address, date_of_birth\n",
    "- Process the csv file to anonymise the data\n",
    "- Columns to anonymise are first_name, last_name and address\n",
    "- You might be thinking that is silly\n",
    "- Now make this work on 2GB csv file (should be doable on a laptop)\n",
    "- Demonstrate that the same can work on bigger dataset\n",
    "- Hint - You would need some distributed computing platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a42b2-1121-465b-ad4c-c2f44f4e5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to check and install necessary packages in Jupyter Notebook\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# List of packages to check and install\n",
    "packages = [\n",
    "    'pandas',\n",
    "    'faker',\n",
    "    'pyspark'\n",
    "]\n",
    "\n",
    "# Function to check and install packages\n",
    "def install_packages(pkg_list):\n",
    "    for pkg in pkg_list:\n",
    "        try:\n",
    "            importlib.import_module(pkg)\n",
    "            print(f'{pkg} is already installed.')\n",
    "        except ImportError:\n",
    "            print(f'{pkg} not found. Installing...')\n",
    "            # Install the package using pip\n",
    "            !pip install {pkg}\n",
    "\n",
    "# Run the function\n",
    "install_packages(packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1ea52-e542-41c4-995e-120f710b28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import faker\n",
    "import hashlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sha2, col\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b919e-7c41-4f15-845c-8ba628ecb4a7",
   "metadata": {},
   "source": [
    "### Step 1: Generate a CSV File\n",
    "\n",
    "Creating a sample CSV file with the following columns:\n",
    "- `first_name`\n",
    "- `last_name`\n",
    "- `address`\n",
    "- `date_of_birth`\n",
    "\n",
    "This CSV file will serve as the input for subsequent data processing steps. Ensurring that the file contains a sufficient amount of data for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287e98a-c7a1-4762-8930-e598e3778c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker to generate fake data\n",
    "fake = faker.Faker()\n",
    "\n",
    "# Number of rows to generate\n",
    "num_rows = 10**6  # 1 million rows\n",
    "chunk_size = 10**5  # Process in chunks of 100,000 rows\n",
    "\n",
    "# Function to generate a chunk of data\n",
    "def generate_chunk(start_index, end_index):\n",
    "    data = {\n",
    "        'first_name': [fake.first_name() for _ in range(start_index, end_index)],\n",
    "        'last_name': [fake.last_name() for _ in range(start_index, end_index)],\n",
    "        'address': [fake.address().replace('\\n', ', ') for _ in range(start_index, end_index)],\n",
    "        'date_of_birth': [fake.date_of_birth(minimum_age=18, maximum_age=90) for _ in range(start_index, end_index)],\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Write to CSV in chunks\n",
    "csv_file_path = 'large_dataset.csv'\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "    # Create header\n",
    "    header_written = False\n",
    "\n",
    "    # Generate and write data in chunks\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for start in range(0, num_rows, chunk_size):\n",
    "            end = min(start + chunk_size, num_rows)\n",
    "            futures.append(executor.submit(generate_chunk, start, end))\n",
    "\n",
    "        for future in futures:\n",
    "            df_chunk = future.result()\n",
    "            df_chunk.to_csv(file, mode='a', header=not header_written, index=False)\n",
    "            header_written = True\n",
    "\n",
    "print(f\"Data generation complete. CSV file saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b9daf-9008-4877-9a17-464747a1f42d",
   "metadata": {},
   "source": [
    "### Step 2: Load and Preview the CSV File\n",
    "Load the generated CSV file and preview the data to ensure it has been generated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009d487-736d-4c63-a753-1b776dd81171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Preview the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac7d89-0097-4931-9a24-cf3b44951766",
   "metadata": {},
   "source": [
    "### Step 3: Anonymize Data\n",
    "Anonymize the first_name, last_name, and address columns using hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b08583-50ca-4132-ba1b-b476e80d6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_data(value):\n",
    "    return hashlib.sha256(value.encode()).hexdigest()\n",
    "\n",
    "# Apply anonymization\n",
    "df['first_name'] = df['first_name'].apply(anonymize_data)\n",
    "df['last_name'] = df['last_name'].apply(anonymize_data)\n",
    "df['address'] = df['address'].apply(anonymize_data)\n",
    "\n",
    "# Preview the anonymized data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01b571-b1c9-4372-ac9d-a4d8986eb625",
   "metadata": {},
   "source": [
    "### Step 4: Save Anonymized Data\n",
    "Save the anonymized DataFrame to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9468f-5e32-4a95-ad66-232cc31d2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the anonymized CSV file\n",
    "anonymized_csv_file_path = 'anonymized_large_dataset.csv'\n",
    "\n",
    "try:\n",
    "    df.to_csv(anonymized_csv_file_path, index=False)\n",
    "    print(f\"Anonymized data successfully saved to {anonymized_csv_file_path}.\")\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during the file writing process\n",
    "    print(f\"Failed to save anonymized data to CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1aecab-1753-4499-9e32-d822522a2583",
   "metadata": {},
   "source": [
    "### Step 5: Handling Large Datasets\n",
    "For large file or 2GB file, can use chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4611a-e52b-4390-80b7-b444e8209ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10**5  # Process 100,000 rows at a time, can change as required. \n",
    "\n",
    "with pd.read_csv(csv_file_path, chunksize=chunk_size) as reader:\n",
    "    for i, chunk in enumerate(reader):\n",
    "        # Anonymize the chunk\n",
    "        chunk['first_name'] = chunk['first_name'].apply(anonymize_data)\n",
    "        chunk['last_name'] = chunk['last_name'].apply(anonymize_data)\n",
    "        chunk['address'] = chunk['address'].apply(anonymize_data)\n",
    "        \n",
    "        # Save the chunk to a new file\n",
    "        if i == 0:\n",
    "            chunk.to_csv(anonymized_csv_file_path, mode='w', index=False)\n",
    "        else:\n",
    "            chunk.to_csv(anonymized_csv_file_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62429355-2618-46a0-bbd1-c8ac93a12070",
   "metadata": {},
   "source": [
    "### Step 6: Scaling with Distributed Computing\n",
    "For larger datasets, this process can be scaled using a distributed computing platform like Apache Spark. We can define inpute, process and output, general ETL process or medallion architecture, however, here I have used same large_dataset.csv generated in above steps and process it to anonymize or hashed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f5a3c-fee4-490a-8c30-f79ad5f439cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "csv_file_path = 'large_dataset.csv'\n",
    "hashed_csv_file_path = 'hased_large_dataset.csv'\n",
    "\n",
    "try:\n",
    "    # Initialize Spark session\n",
    "     spark = SparkSession.builder.appName(\"HashedData\").getOrCreate()\n",
    "  \n",
    "    # Load the CSV file into a Spark DataFrame\n",
    "    df_spark = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "    \n",
    "\n",
    "    # Anonymize the columns\n",
    "    df_spark = df_spark.withColumn(\"first_name\", sha2(col(\"first_name\"), 256))\n",
    "    df_spark = df_spark.withColumn(\"last_name\", sha2(col(\"last_name\"), 256))\n",
    "    df_spark = df_spark.withColumn(\"address\", sha2(col(\"address\"), 256))\n",
    "\n",
    "    # Save the anonymized DataFrame back to a CSV\n",
    "    df_spark.write.mode(\"overwrite\").csv(hashed_csv_file_path, header=True)\n",
    "\n",
    "    print(f\"Hashed data successfully saved to {hashed_csv_file_path}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any exceptions that occur during Spark operations\n",
    "    print(f\"Failed to process and save hashed data: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Ensure Spark session is stopped\n",
    "    try:\n",
    "        spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "    except:\n",
    "        print(\"Failed to stop the Spark session.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9d55f-5da5-4062-9da6-c028e5eb6c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
